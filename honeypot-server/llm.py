import os
import json
import time
import yaml
import asyncio
import re
import logging
from collections import OrderedDict
from openai import AsyncOpenAI

# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("LLM_Honeypot")

# =============================
#    Resilience Components
# =============================

class CircuitBreaker:
    def __init__(self, fail_threshold=3, reset_time=30):
        self.fail_threshold = fail_threshold
        self.reset_time = reset_time
        self.fail_count = 0
        self.last_fail_time = 0
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    def allow_request(self):
        if self.state == "OPEN":
            if time.time() - self.last_fail_time > self.reset_time:
                self.state = "HALF_OPEN"
                logger.info("Circuit Breaker entering HALF_OPEN state.")
                return True
            return False
        return True

    def record_success(self):
        if self.state != "CLOSED":
            logger.info("Circuit Breaker recovered. Resetting to CLOSED.")
        self.fail_count = 0
        self.state = "CLOSED"

    def record_failure(self):
        self.fail_count += 1
        self.last_fail_time = time.time()
        logger.warning(f"Failure detected. Count: {self.fail_count}/{self.fail_threshold}")
        
        if self.fail_count >= self.fail_threshold:
            self.state = "OPEN"
            logger.error(f"Circuit Breaker TRIPPED. Pausing for {self.reset_time}s.")

class TTLCache:
    def __init__(self, ttl_seconds=60, max_size=100):
        self.cache = OrderedDict()
        self.ttl = ttl_seconds
        self.max_size = max_size

    def get(self, key):
        if key not in self.cache:
            return None
        
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
        
        self.cache.move_to_end(key)  # Mark as recently used
        return value

    def set(self, key, value):
        if len(self.cache) >= self.max_size:
            self.cache.popitem(last=False)  # Remove oldest
        self.cache[key] = (value, time.time())

# =============================
#     Helper / Loading Code
# =============================

MODULE_DIR = os.path.dirname(os.path.abspath(__file__))

def load_system_prompt():
    path = os.path.join(MODULE_DIR, 'personalitySSH.yml')
    try:
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get('personality', {}).get('prompt', "You are a Linux server.")
    except Exception as e:
        logger.error(f"Failed to load personality: {e}")
    return "You are a Linux server. Reply only with terminal output."

def load_default_examples():
    path = os.path.join(MODULE_DIR, "fewshots.json")
    fallback = [{"command": "whoami", "response": "root"}]
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return fallback

DEFAULT_FEW_SHOT_EXAMPLES = load_default_examples()

def build_few_shot_prompt(system_prompt, examples, user_input):
    parts = [system_prompt.strip(), ""]
    for ex in examples:
        parts.append(f"Input: {ex.get('command')}\nOutput: {ex.get('response')}")
    parts.append("### Task")
    parts.append(f"Input: {user_input.strip()}")
    return "\n".join(parts)

# =============================
#           LLM Class
# =============================

class LLM:
    def __init__(self, api_key=None, api_model=None, max_examples=None, max_retries=3):
        self.api_model = api_model or os.getenv("MODEL_NAME") or "gemini-2.0-flash"
        self.examples = DEFAULT_FEW_SHOT_EXAMPLES[:max_examples] if max_examples else DEFAULT_FEW_SHOT_EXAMPLES
        self.max_retries = max_retries
        self.system_prompt = load_system_prompt()

        key = api_key or os.getenv("GEMINI_API_KEY") or os.getenv("API_KEY")
        if not key:
            raise ValueError("API Key missing. Set GEMINI_API_KEY in .env")

        # Initialize Async Client
        self.client = AsyncOpenAI(
            api_key=key,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
        )

        # Resilience Layers
        self.circuit = CircuitBreaker(fail_threshold=3, reset_time=20)
        self.cache = TTLCache(ttl_seconds=300, max_size=200)

        logger.info(f"LLM initialized. Model: {self.api_model}")

    def _sanitize(self, text: str) -> str:
        """
        Removes markdown code blocks, bolding, and keeps output looking like raw terminal text.
        """
        if not text: return ""
        text = str(text)
        
        # 1. Remove code block markers (```bash, ```, etc)
        text = re.sub(r'```[a-zA-Z]*', '', text)
        
        # 2. Remove inline code backticks
        text = text.replace('`', '')
        
        # 3. Remove Markdown bold/italic
        text = re.sub(r'\*\*|__|\*|_', '', text)
        
        return text.strip()

    async def answer(self, query, log_history=None):
        if log_history is None: log_history = []
        
        # 1. Cache Check
        cache_key = f"{query}::{len(log_history)}"
        cached_resp = self.cache.get(cache_key)
        if cached_resp:
            logger.info(f"Cache Hit for: {query[:10]}...")
            return cached_resp

        # 2. Circuit Breaker Check
        if not self.circuit.allow_request():
            logger.warning("Request blocked by Circuit Breaker.")
            return "Connection timed out"

        # 3. Construct Payload
        prompt_content = build_few_shot_prompt(self.system_prompt, self.examples, query)
        messages = [{"role": "user" if i % 2 == 0 else "assistant", "content": m} for i, m in enumerate(log_history)]
        messages.append({"role": "user", "content": prompt_content})

        # 4. Execute with Retries
        for attempt in range(1, self.max_retries + 1):
            try:
                completion = await self.client.chat.completions.create(
                    model=self.api_model,
                    messages=messages,
                    max_tokens=1024,
                    temperature=0.0, # Low temp for consistent terminal output
                )
                
                raw_text = completion.choices[0].message.content
                clean_text = self._sanitize(raw_text)
                
                # Success: Update State
                self.circuit.record_success()
                self.cache.set(cache_key, clean_text)
                
                return clean_text

            except Exception as e:
                logger.error(f"Attempt {attempt} failed: {e}")
                self.circuit.record_failure()
                if attempt < self.max_retries:
                    await asyncio.sleep(0.5 * attempt) # Exponential backoff

        return "Internal Server Error"